{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Certified Machine Learning - Specialty\n",
    "\n",
    "This notebooks include an outline of the key AWS services and techniques to know for the certification exam. More details please refer to AWS websites. Part of the materials can also be found from the **aws-cloud-practitioner-essentials notebooks** saved in this repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 1: Data Engineering\n",
    "### AWS Kinesis\n",
    "#### Kinesis Streams\n",
    "- Use case: Real-time application\n",
    "- Streams limit \n",
    "    - Producer: 1MB/s or 1000 messages/s at write per shard\n",
    "    - Consumer: 2MB/s at read per shard and 5 API calls per second per shard across all consumers\n",
    "    \n",
    "#### Kinesis Firehose\n",
    "- Use case: Data ingestion\n",
    "\n",
    "#### Kinesis Analytics\n",
    "- Use case: Streaming ETL, continuous metric generation, responsive analytics\n",
    "- Machine learning functions:\n",
    "    - RANDOM_CUT_FOREST fo anomaly detection\n",
    "    - HOTSPOTS\n",
    "    \n",
    "#### Kinesis Video Streams\n",
    "\n",
    "### Glue \n",
    "#### Glue Data Catelog, Glue Crawlers, Glue ETL\n",
    "- Glue ETL runs Run Apache Spark code\n",
    "\n",
    "### AWS Data Stores\n",
    "#### Redshift\n",
    "- Data warehouse\n",
    "- SQL analytics, OLAP - online analytical processing\n",
    "\n",
    "#### RDS, Aurora\n",
    "- Relational database\n",
    "- OLTP - Online Transaction Processing\n",
    "\n",
    "#### DynamoDB\n",
    "- NoSQL data store\n",
    "\n",
    "#### S3\n",
    "- Objectie store\n",
    "- S3 storage tiers\n",
    "- S3 encryption (using KMS)\n",
    "- S3 security (using relevant AWS services IAM policeis)\n",
    "\n",
    "#### ElasticSearch\n",
    "#### ElasticCache\n",
    "\n",
    "### AWS Data Pipeline\n",
    "- Orchestration service\n",
    "\n",
    "### AWS Batch\n",
    "- Run batch jobs as Docker images\n",
    "\n",
    "### AWS DMS (Database Migration Service)\n",
    "\n",
    "### AWS Step Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 2: Exploratory Data Analysis\n",
    "### Amazon Athena\n",
    "- Pay-as-you-go: 5 dollar per TB scanned\n",
    "\n",
    "### Amazon QuickSight\n",
    "- Data visualization tool for everyone\n",
    "- 10 GB of super-fast, parallel, in-memory calculated (SPICE) engine\n",
    "- Machine learning features: anomaly detection, forecasting, auto-narratives\n",
    "\n",
    "### EMR (Elastic MapReduce)\n",
    "- Hadoop framework on EC2 instances\n",
    "- Nodes\n",
    "- Storage\n",
    "- Spark\n",
    "- Zeppeline\n",
    "- EMR notebook\n",
    "- Instance types: m4.large if < 50 nodes, m4.xlarge if > 50 nodes for master node\n",
    "- Spot instances are good choice for task nodes\n",
    "\n",
    "### SageMaker Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 3: Modeling\n",
    "\n",
    "### Deep learning on EC2 / EMR\n",
    "- EMR supports Apache MXNet and **GPU** instance types\n",
    "\n",
    "\n",
    "### Use SageMaker through SageMaker Notebooks\n",
    "#### 1. Open SageMaker on AWS Management Console\n",
    "#### 2. Create a notebook instance and set up the following accordingly. (The notebook instance is different from the sagemaker instances that train or host the model.)\n",
    "- Name and instance type\n",
    "- Permission and encryption - IAM role \n",
    "- Network - VPC\n",
    "- Git repository\n",
    "- Tags\n",
    "\n",
    "\n",
    "#### 3. Open Jupyter from the notebook instance \n",
    "#### 4. Create/upload python training script \n",
    "```python\n",
    "if '__name__' == '__main__':\n",
    "   ### Training codes ###\n",
    "\n",
    "# Save model\n",
    "```\n",
    "    \n",
    "#### 5. In sagemaker notebook, import estimators from sagemaker, create an estimator with the training script, and fit the model. (A Docker container is then started with a SageMaker instance that trains the model and a docker image registered on ECR.)\n",
    "```python\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "ts_estimator = TensorFlow(entry_point='train.py',\n",
    "                          role=role,\n",
    "                          train_instance_count=1,\n",
    "                          trian_instance_type='local', # or use AWS instance Ex: m1.p3.2xlarge\n",
    "                          frameworkf_version='1.12',\n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={})\n",
    "\n",
    "ts_estimator.fit({'training': s3_training_input_path, 'validation': s3_validation_input_path})\n",
    "\n",
    "```\n",
    "##### Pre-built models are docker images. The training options include:\n",
    "- Built-in training algorithms\n",
    "- scickit-learn\n",
    "- Spark MLlib\n",
    "- Tensorflow, MXNet, Chainer, PyTorch\n",
    "- Custom Python Tensorflow / MXNet code\n",
    "- Your own Docker image\n",
    "- Algorithm purchased from AWS\n",
    "\n",
    "#### 6. Deploy the model after training (A Docker container is then started with a SageMaker instance that hoss the model and a docker image registered on ECR. ***In addition, an endpoint is created for the model.***)\n",
    "```python\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1,\n",
    "                                   instance_type='m1.c5.large',\n",
    "                                   accelerator_type='ml.eia1.medium',\n",
    "                                   endpoint_name=tf_endpoint_name)\n",
    "```\n",
    "\n",
    "### SageMaker Built-In Algorithms\n",
    "\n",
    "#### 1. Classification and Regression\n",
    "- Linear Learner - CPU or GPU\n",
    "- XGBoost - CPU\n",
    "```python\n",
    "Sagemaker.xgboost\n",
    "```\n",
    "- KNN - CPU or GPU\n",
    "    - Dimensional reduction stage included\n",
    "\n",
    "#### 2. Deep Learning\n",
    "- Seq2Seq - GPU only\n",
    "    - From a sequence of tokens to sequence of tokens with CNN and RNN\n",
    "- DeepAR - CPU or GPU\n",
    "    - Forecasting one-dimensional time series data with RNN\n",
    "- BlazingText\n",
    "    - Text classification: supervised; predict labels on a sentence \n",
    "    - Word2vec\n",
    "- Object2Vec\n",
    "    - Unsupervised; clustering\n",
    "- Object Detection - GPU; CPU or GPU for inference\n",
    "    - Identify all objects in an image\n",
    "- Image Classification - GPU for training; CPU or GPU for inference\n",
    "    - Assign one or more labels to an image\n",
    "- Semantic Segmentation - GPU for training; CPU or GPU for inference\n",
    "    - Pixel-level object classification\n",
    "    \n",
    "#### 3. Unsupervised Machine Learning\n",
    "- Random Cut Forest - CPU (M4, C4, C5)\n",
    "    - Anomaly detection\n",
    "- Neural Topic Model - CPU or GPU\n",
    "    - Unsupervised; Topic modeling by Neural Variational Inference\n",
    "- LDA (Latent Dirichlet Allocation) - CPU\n",
    "    - Unsupervised; Topic modeling\n",
    "- K-Means - CPU or GPU\n",
    "    - Large scale web-scale clustering is available\n",
    "- PCA - CPU or GPU\n",
    "    - Singular value decomposition (SVD)\n",
    "- Factorization Machines - CPU or GPU\n",
    "    - Pair-wise data; Recommendation systems\n",
    "- IP Insights - CPU or GPU\n",
    "    - Unsupervised; Identify suspicious behavior from IP addresses\n",
    "\n",
    "#### 4. Reinforced Learning\n",
    "- Reinforcement Learning\n",
    "    - Q-Learning\n",
    "    - Markov Decision Process\n",
    "\n",
    "### Automatic Model Tuning with SageMaker\n",
    "```Python\n",
    "```\n",
    "\n",
    "### SageMaker + Spark\n",
    "```python\n",
    "sagemaker_pyspark\n",
    "sagemaker_pyspark.algorithms\n",
    "sagemaker_pyspark.transformation\n",
    "```\n",
    "\n",
    "### High-Level AL/ML Services\n",
    "- Amazon Comprehend\n",
    "    - NLP and text analytics: entities, key phrases, language, sentiment, and syntax\n",
    "- Amazon Translate\n",
    "- Amazon Transcribe\n",
    "    - Speech to text, speaker identification, and channel identification\n",
    "- Amazon Polly\n",
    "    - Text to speech\n",
    "- Rekognition\n",
    "    - Images or videos\n",
    "- Amazon Forecast\n",
    "    - Time-series analysis with ARIMA, DeepAR, ETS, NPTS, and Prophet\n",
    "- Amazon Lex\n",
    "    - Natural-language chatbot engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain 4: Machine Learning Implementation and Operations\n",
    "\n",
    "All models in SageMaker are hosted in Docker containers. Containers are isolated and contain all dependencies and resources needed to run. ***Docker containers are created from images, which are built from a Dockerfile and saved in a repository, such as Amazon Elastic Container Registry (ECR).***\n",
    "\n",
    "### Production Variants\n",
    "\n",
    "### SageMaker Neo + AWS IoT Greengrass\n",
    "Greengrass the model compiled by Neo to edge devices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
